Engineering data sets (in-class tutorial)
========================================================
author: Hannes Datta
date: 
autosize: true

<style>
.small-code pre code {
  font-size: 1em;
}
</style>


Before we get started
=====================

- Exam dates are now available
  - dPrep: 3 April 2025 (resit 27 June)
  - oDCM: 1 April 2025 (resit 26 June)
- Mac users: Please familiarize yourselves with R/RStudio on Windows during the oDCM lab session this Friday

Structure of this week
========================================================

- __Today's (updated & overhauled) tutorial:__ back to R, __but__: maintain way of working using Git/GitHub
  - Part 1: expanding our knowledge of R
      - applying functions, advanced cleaning, dealing with time, merging data, pivoting
  - Part 2: case studies and exercises
      - application to typicaly marketing analytics data issues
      - some exercises still to be added
- Coaching session
- After class
  - Work through the [tutorial](https://dprep.hannesdatta.com/content/docs/modules/week4/tutorial/data-preparation.html) (self study)
  - Optional content: [Tidyverse - chapter 1 & 3](https://datacamp.com/courses/introduction-to-the-tidyverse/data-wrangling-1?ex=1), [Cleaning data in R - chapter 1 & 2](https://datacamp.com/courses/cleaning-data-in-r), [Joining Data with `dplyr`- chapter 1 & 2](https://datacamp.com/courses/joining-data-with-dplyr/joining-tables-1)

__Get your cheat sheets ready!!!__


Why this tutorial
==================
incremental: true

- Final data set rarely available
- Requires wrangling/transformations (can you name a few examples? there are *many*!)
- In projects, structure scripts in four blocks -- required to automate workflow later
  - Setup/prerequisites/loading packages (`library()`)
  - Input (e.g., `read_csv()`) - always __relative__ filenames!
  - Transformation (i.e., operations on your data)
  - Output (e.g., `write_csv()` -- also w/ __relative__ filenames)
- __Now:__ zoom in on ways to transform (while respecting setup-I-T-O building blocks)

Recap: Commands you already know 
=========

- `select()`: subset columns
- `filter()`: subset rows on conditions
- `mutate()`: create new columns by using information from other columns
- `group_by()` and `summarize()`: create summary statistics on grouped data
- `arrange()`: sort results
- `count()`: count discrete values



DO: Let's get today's data and start exploring
======
class: small-code

Run this snippet to download today's data (it's zipped - contains a couple of `.csv` files)

```{r}
library(tidyverse)
download.file('https://github.com/hannesdatta/course-dprep/raw/refs/heads/new_week4/content/docs/modules/week4/tutorial/data/week4_data.zip', 'week4_data.zip')
unzip('week4_data.zip', overwrite=TRUE)
```

- Load all data sets using `read_csv()`
- Then, use the `head()`, `View()`, `summary()`, `dim()` commands and try to describe the data set

Solution
====

```{r}
library(tidyverse)

streams <- read_csv('streams.csv')
weather <- read_csv('weather.csv')
socials <- read_csv('socials.csv')
playlists <- read_csv('playlists.csv')
```

Part 1: Expanding our knowledge of R
=====

- Knowledge so far is restricted to...
  - opening/saving data (`read_csv`, `write_csv`), 
  - exploring data (`head`, `View()`, `summary()`, `table()`, `dim()`
  - wrangling with tidyverse/dplyr: `group_by()`, `summarize()`, `mutate()`, `filter()`, `select()`, `arrange()`, `count()`
  
- But... what about more complex operations?
  - custom summary statistics? ranking? 
  - removing duplicates?
  - complex text searches?
  - dealing with time series data?
  - merging multiple data sets?
  - reshaping data?
  
Part 1: Expanding our knowledge of R
=====

What's to come...

- 1A) Applying base R functions in `tidyverse`
- 1B) Advanced Cleaning: Using regular expressions
- 1C) Dealing with Time
- 1D) Merging Data
- 1E) Reshaping Data

1A) Applying functions in `tidyverse`
=====
class: small-code

- Summarize can take any functions as arguments
- Best to have Base R cheat sheet available

```{r}
streams %>%
  group_by(artist) %>%
  summarize(
    mean_streams = mean(streams, na.rm = TRUE))
```

DO: Extend code snippet above to include standard deviation (`sd`), minimum (`min`) and maximum (`max`) in the summary table.

Solution
====

```{r eval=FALSE}
library(dplyr)

# Compute basic statistics per artist
streams %>%
  group_by(artist) %>%
  summarize(
    min_streams = min(streams, na.rm = TRUE),
    mean_streams = mean(streams, na.rm = TRUE),
    max_streams = max(streams, na.rm = TRUE),
    sd_streams = sd(streams, na.rm = TRUE) 
    
  )
```

1A) Applying functions in `tidyverse`
=====
class: small-code

__DO__ 

- Extract the daily "top 10" - i.e., the top 10 artists on a given day.
- Tips
  - First, group by `date` and `country`
  - Use the `rank()` function to find the "rank" of streams (inverse!) - in combination w/ `mutate()`
  - Reorder your data set by `date` and `ranks`
  - Show the first 10 observations for each day (`filter`)

Solution
====

```{r}
streams %>% 
  group_by(date, country) %>% 
  mutate(rank = rank(-streams)) %>% 
  ungroup() %>% 
  arrange(date, country, rank) %>% 
  filter(rank<=10)

```

1A) Applying functions in `tidyverse`
=====
class: small-code

- Meeting `unique()`: removes duplicates from a vector
- `n_distinct()`: counts the number of unique values in a vector
- `distinct()`: removes duplicate rows
- Let's count the number of unique artists

```{r}
streams %>% summarize(number_of_artists = n_distinct(artist))

# or: length(unique(streams$artist))

distinct(streams)
```

1B) Advanced Cleaning: Using Regular Expressions
=====

- Functions you need to know
  - `grepl(x, y)` - detect whether `y` contains `x` (evaluates to `TRUE` or `FALSE`)
  - `grep(x, y, value=TRUE)` - detect whether `y` contains `x` (& returns the search results)
  - `gsub(x, z, y)` - searches `x` in `y` and replaces `x` by `z`
- How to search? Examples:
  - `grepl("\\d", "Song123")` &rarr; `TRUE`
  - `grepl("[a-z]", "K3")` → `FALSE` (case-sensitive)
  - `grepl("^The", "The Beatles")` → `TRUE`, but...
  - `grepl("^the", "The Beatles")` → `FALSE` (know why?)
  - `grepl("band$", "Jazz Band", ignore.case=T)` → `TRUE` 
  - `grepl("jazz$", "Jazz Band", ignore.case=T)` → `FALSE` (know why?)
  

1B) Advanced Cleaning: Using Regular Expressions
=====

__DO__:

- How many artists have a number in their name?
- Which bands end on "Band"?
- Replace `Storm` by `Wind`

Tip: first extract the unique artist names.

```{r eval=T, include=TRUE}
artists <- unique(streams$artist)

```


Solution
===

```{r eval=F, include=TRUE}
artists <- unique(streams$artist)
grep('\\d', artists, value=T)
grep('band$', artists, value=T, ignore.case=T)
gsub('Storm','Wind',artists)

```

1C) Dealing with time
=====

- `zoo::na.locf()`

```{r}
playlists = playlists %>% group_by(artist) %>% arrange(artist, iso_week) %>% mutate(playlists_imputed = zoo::na.locf(playlists, na.rm=F))
playlists
```


1C) Dealing with time
=====

- leads and lags

```{r, eval=F, echo=T}
playlists = playlists %>% group_by(artist) %>% arrange(artist, iso_week) %>% mutate(playlists_lagged = lag(playlists_imputed),
       playlists_lead = lead(playlists_imputed))

playlists
```

__DO:__ Can you compute the change from one week to the next?

Solution
====

```{r}
playlists = playlists %>% group_by(artist) %>% arrange(artist, iso_week) %>% mutate(playlists_lagged = lag(playlists_imputed),
       playlists_lead = lead(playlists_imputed),
       change = playlists_imputed - playlists_lagged)
playlists
```

1C) Dealing with time
=====

- Time conversion using `lubridate`
  - `isoweek()`
  - `month` and `year` 
  - potentially to use w/ `paste0()`

- DO: create a week variable from the date column
  

Solution
====

```{r}
library(lubridate)

streams <- streams %>%
  mutate(
    week = isoweek(date),
    week2 = ISOweek::ISOweek(date),
    month = month(date),
    year = year(date)
  )

streams %>% select(date, week, week2, month, year) %>% head(10)
```

1D) Merging/joining
=======

- How can we assemble different data sets together? 
- We can *join* them!
  - Why join / Why not to join? Mostly a memory issue. Think about it wisely.
  - Keys: required to join (e.g., single key, composite keys
  - Different type of joins
       - `left_join` ("keep everything in the left table, add the right table where you can)
       - `inner_join()` - keeps only observations that occur in *both tables* (potential of losing control!)
  
Do: Let's join!
=====
class: small-code

1. Create a ISO week column for streams (from date)
2. Join streams (left) with playlists (right) using a `left_join()`; join by artist and week (composite key).

Solution
====

```{r}
streams <- streams %>% mutate(week=ISOweek::ISOweek(date))

merged_data <- streams %>%
  left_join(playlists, by = c("artist" = "artist", 
                              "week"="iso_week"))
head(merged_data)

```


1E) Pivoting
=======

- Mostly two data set "types"
  - "Long": each observation has its own row, with a single column for values (see `streams.csv`)
  - "Wide": each row is a single entity, with multiple columns for different variables (see `weather.csv`)
- When we convert from long to wide or the other way around, we call this 'pivoting' or 'reshaping'

1E) Pivoting: wide to long
=====

Requires: `cols`, `names_to`, and `values_to`

```{r}

long = weather %>% pivot_longer(cols=!date, names_to='variable',values_to='value')
long
```

1E) Pivoting: long to wide
=====

Requires: `id_cols`, `names_from`, `values_from`
```{r}
long %>% pivot_wider(id_cols=c('date'), names_from='variable',values_from='value')

```

Part 2: Case studies (work in progress)
====

1. Aggregate `streams` to the weekly level (ISO)
2. Create a trend variable, going from 1 (for the first date) to the last date
3. Full data set: for all artists, countries, and dates, create an "empty" data set; then, merge existing `streams` to it.
4. Do LOCF imputation on `streams` in (3)
5. Load weather data, pivot to long by country & merge (by date & country)
6. Load social media data, pivot to wide and merge
7. Scaling up I: use `for` loop to create 10 line plots for an artists' streams, and save using `ggsave()`
8. Scaling up II: use `lapply` to estimate individual regression models (`lm`) for 10 artists; return results and summarize.


Summary of today's tutorial
=====
incremental: true

- Work in "ITO building blocks" (+ loading libraries, of course!)
- Wrangling is necessary to get data into shape and generate meaning/understanding
- Pivoting (from long to wide, and wide to long) is difficult... it's enough if you get the concept now - you will have to fiddle around with it a bit anyways
- Have your cheat sheets available always -- especially the `dplyr` one is useful (and very [visual](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)!)



Next steps 
===============

- Complete exercises in this tutorial
  - selected solutions available in the `.Rpres` file ([available on GitHub](https://github.com/hannesdatta/course-dprep/tree/master/content/docs/modules/week4/tutorial))

- You will continue with the coaching session


Coaching notes
==========

- absolutely important to work in local repositories, with issue-based branches, and pull requests.
- Source code files need to be structured as setup-ITO -- required for automation!
- Choose appropriate file format: Rmarkdowns or Quarto for reports/PDFs/HTML, `.R` for regular scripts
- Adhere to `tidyverse` functions (e.g., `read_csv()`, not `read.csv()`)
- Incorporate programming concepts where needed: __looping__, using __functions__

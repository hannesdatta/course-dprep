---
title: "Data Preparation"
date: "Last updated: 12 February 2025"
output:  
  webexercises::webexercises_default:
    theme: flatly
    highlight: tango
    toc: true
---



```{r setup, include=FALSE}
#output: webexercises::webexercises_default
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(webexercises)
library(knitr)

```

# Introduction

In this tutorial, we'll tackle common data challenges using a simulated music streaming dataset. Starting with multiple raw data sources, we'll clean, transform, and merge them into a single analysis-ready data frame.

## What You'll Learn in this Tutorial

- Expand your knowledge of R functions (e.g., base functions such as `rank()` or `unique()`, `for` loops and `lapply()`)
- Merge data sets on one or more indices and understand the different types of joins (inner, left, right join).
- Handle missing and duplicate values.
- Handle and preprocess string data (remove characters, splitting using regular expressions).
- Operationalize variables and handle pivoting (wide to long, long to wide)
- Scale workflows (e.g., batch plotting, estimation of linear models, `lm()`)
- Implement techniques to exemplary case studies often encountered in marketing research

## Prerequisites

Ready to start? Here's what we recommend you do before going through this tutorial.

* [Datacamp Introduction to Tidyverse](https://datacamp.com/courses/introduction-to-the-tidyverse/data-wrangling-1?ex=1) (chapter 1 and 3)
* [Datacamp Cleaning Data in R](https://datacamp.com/courses/cleaning-data-in-r) (chapter 1 and 2)
* [Datacamp Joining Data with dplyr](https://datacamp.com/courses/joining-data-with-dplyr/joining-tables-1) (chapter 1 and 2)


## Downloading and Exploring the Data

Let's explore the data. It consists of four files, which vary in structure, unit of observation, etc. This is on purpose because then you can learn all the concepts required in this tutorial.
 
- `artist_streams.csv`: number of streams (`streams`) for an `artist` in a given `country` on a given day (`date`)
- `weather.csv`: for each day and country, the average temperature (`_temp`) and sun hours (`_sun`)
- `socials.csv`: for every `artist` and day (`date`), this data lists social media metrics (e.g., on `platform` TikTok, a `metric` followers is observed and has the `value` 458352)
- `playlists.csv`: for each week `iso_week` and `artist`, the data lists the number of `playlists` an artist was listed on on Spotify.

### Create a new R project
1. Open RStudio and create a new project by clicking on `File` -> `New Project` -> `New Directory` -> `New Project`.
2. Name your project `data-preparation` and save it in a location of your choice.
3. Add a `data` folder:
- Option 1 (code): Run `dir.create("data")
- Option 2 (Manual): Click *New Folder (üìÅ)* in the Files pane  

### Download the data

Click [here](https://github.com/hannesdatta/course-dprep/raw/master/content/docs/modules/week4/tutorial/data_without_duplicates.zip) to manually download the dataset. Alternatively, run the following R code to download and unzip the file automatically: 

```{r}
# downloading and unzipping the data
#download.file('https://github.com/hannesdatta/course-dprep/raw/master/content/docs/modules/week4/tutorial/data_without_duplicates.zip', 'data.zip')
#unzip('data.zip', overwrite=TRUE)
```
Now, load the data into R using `read_csv`:


```{r echo=TRUE, message=FALSE, results='hide'}
library(tidyverse)

streams <- read_csv('data/artist_streams.csv')
weather <- read_csv('data/weather.csv')
socials <- read_csv('data/socials.csv')
playlists <- read_csv('data/playlists.csv')

```

__Troubleshooting__: 

- If R doesn‚Äôt find your file: Click **Session ‚Üí Set working directory ‚Üí To source file location**. Avoid using `setwd()` ‚Äî hardcoding directories can cause problems!

### Explore the data

We can now explore the data using the strategies introduced in earlier tutorials.

1. What is the unit of observation? (*The unit of analysis tells us what each row in the dataset represents.*)
2. Are there any missing values in the columns? Do values make sense?
3. What are common columns across the datasets that could be used for merging?

__Tips:__

- Take a __look__ at the data using the function `head(streams)`, or `View(streams)`. If you'd like to view more rows with `head`, use `head(streams, 100)` (for, e.g., the first 100 rows). (`tail()` gives you the last rows of the data).
- The command `summary(playlists)` or `glimpse(playlists)` generates descriptive statistics for all variables in the data. You can also use this command on individual columns (e.g., `summary(playlists$playlists)`).
- Character or factor columns are best inspected using the `table()` command. These will create frequency tables.

```{r eval=FALSE, include=TRUE, webex.hide='Solutions'}

# View the first rows of the datasets to get an overview
head(streams)
head(socials)
head(weather)
head(playlists)

glimpse(streams)
glimpse(socials)
glimpse(weather)
glimpse(playlists)

```

# Part 1: Expanding our knowledge of R

In this section, we will deepen our understanding of key R functions that are essential for data engineering. We will focus on numerical functions, advanced string cleaning, and working with time-based data. Each concept will be demonstrated with examples from our datasets.

## Applying base R functions in `tidyverse`

Before diving into more advanced transformations, let‚Äôs start by applying **base R functions** inside `dplyr` operations. In `mutate()` and `summarize()`, you can use **any** function from base R‚Äînot just `mean()`, `min()`, or `max()`. This is useful when operationalizing new variables, performing custom transformations, or computing statistics on groups.

For now, we'll use **min, mean, and max** to get started. But remember, the same logic applies to **any** function, like `sd()`, `range()`, `var()`, or even custom functions you define.

### Compute summary statistics for streaming data

```{r}
library(dplyr)

# Compute basic statistics per artist
streams %>%
  group_by(artist) %>%
  summarize(
    min_streams = min(streams, na.rm = TRUE),
    mean_streams = mean(streams, na.rm = TRUE),
    max_streams = max(streams, na.rm = TRUE),
    sd_streams = sd(streams, na.rm = TRUE)  # Standard deviation
    
  )
  
```

Want to go further? **Any function that works on a vector can be used here**‚Äîfor example:

- `var(streams, na.rm = TRUE)` to calculate variance.
- `range(streams, na.rm = TRUE)` to get the min and max in one step.
- `quantile(streams, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)` to get quartiles.

üí° **Tip:** You can check out the [Base R Cheatsheet](https://github.com/rstudio/cheatsheets/raw/main/base-r.pdf) to explore more functions you can integrate into your `tidyverse` workflows.

### Ranking: Identifying the Most-Listened-To Artists

Ranking is **super useful** in our application because we may be interested in identifying **the most-listened-to artist** or tracking changes in ranking over time. Instead of just looking at absolute numbers, ranking allows us to **order values while handling ties**, which is often critical in reporting and analysis.

The `rank()` function assigns ranks based on the values in a column, with **ties receiving the same rank**. By default, it ranks in ascending order, but using `-x` flips the order to descending. This is particularly useful when working with **streams data**, where we want to **rank artists by their daily streaming count**.

#### Example: Rank Artists by Streams Per Day
```{r}
library(dplyr)

# Rank artists within each day
streams %>%
  group_by(date) %>%
  mutate(
    rank_streams = rank(-streams),  # Higher streams = lower rank (1st place)
    rank_streams_ties = rank(-streams, ties.method = "min")  # Handles ties
  ) %>%
  arrange(date, rank_streams) %>%
  select(date, artist, streams, rank_streams) %>%
  head(10)
```

üí° **Other useful ranking functions**:
- `row_number(x)`: Assigns unique sequential numbers without ties.

Using these functions, we can **track trends over time** and understand which artists are **consistently ranking high** in streams. 


### Dealing with Unique Values: Removing Duplicates and Counting Distinct Entries  

When working with data, we often need to **remove duplicate values** or **count unique occurrences**. Whether we're filtering artist names, cleaning up datasets, or summarizing trends, **uniqueness matters**.  

In `tidyverse`, we have a few tools to help:  
- `unique()` **kicks out duplicate values** from a vector or data frame.  
- `distinct()` (from `dplyr`) removes duplicate rows from a data frame.  
- `n_distinct()` **counts** the number of unique values in a column.  

#### Example: Get a List of Unique Artists  
Let‚Äôs start simple‚Äîgetting a list of all the unique artists in our dataset.  

```{r}
unique_artists <- unique(streams$artist)  # Extract unique artist names
head(unique_artists, 10)  # Show a preview
```

You can also use `unique()` inside a `mutate()` statement.

#### Example: Count the Number of Unique Countries

Now, let's count how many different countries appear in our dataset:

```{r}
streams %>%
  summarize(num_unique_countries = n_distinct(country))
```


## Advanced Cleaning: Using Regular Expressions

Regular expressions (**regex**) allow us to search for patterns in text, making it easier to clean and filter data. In R, we use:  

- **`grepl()`** ‚Üí Detect patterns in text (**Find something**)  
- **`gsub()`** ‚Üí Replace matched patterns with new values (**Replace something**)  

### Common Regex Search Patterns

| Pattern | Description | Example |
|---------|------------|---------|
| `\\d`  | Find **numbers** (`0-9`) | `grepl("\\d", "Song123")` ‚Üí `TRUE` |
| `[a-zA-Z]` | Find **letters** (case-sensitive) | `grepl("[a-zA-Z]", "123!")` ‚Üí `FALSE` |
| `^word` | Match **beginning** of a string | `grepl("^The", "The Beatles")` ‚Üí `TRUE` |
| `word$` | Match **end** of a string | `grepl("Band$", "Jazz Band")` ‚Üí `TRUE` |

### `grepl()`: Finding Patterns in Text  

Use `grepl()` to **detect whether a pattern exists** in a column. It returns **TRUE/FALSE**, making it useful for filtering data.  

#### Example: Identify artists with "DJ" in their name  
```{r}
streams %>%
  filter(grepl("DJ", artist)) %>%
  select(artist) %>%
  distinct()
```

#### Example: Find artists whose names contain numbers  
```{r}
streams %>%
  filter(grepl("[0-9]", artist)) %>%
  select(artist) %>%
  distinct()
```

#### Example: Find artists starting with "The" (case-insensitive)  
```{r}
streams %>%
  filter(grepl("^The", artist, ignore.case = TRUE)) %>%
  select(artist) %>%
  distinct()
```

#### Example: Find artists ending with "Band"  
```{r}
streams %>%
  filter(grepl("Band$", artist)) %>%
  select(artist) %>%
  distinct()
```


### `gsub()`: Replacing Patterns in Text  

Use `gsub()` to **modify text by replacing matched patterns**.

#### Example: Clean up underscores in artist names (replace `_` with space)  
```{r}
streams <- streams %>%
  mutate(artist_clean = gsub("_", " ", artist))
```

#### Example: Remove numbers from artist names  
```{r}
streams <- streams %>%
  mutate(artist_clean = gsub("[0-9]", "", artist))
```

## Dealing with Time

### Last Value Carried Forward (`na.locf`)

In time-series data, missing values can be imputed using the last observed value (LOCF). This is useful for forward-filling missing entries. The function is available in the `zoo` package. Please install it if it is unavailable (`install.packages('zoo')`).

#### Example: Fill missing number of playlists

```{r}
library(zoo)

# Let's create a few missing observations
set.seed(1234)
streams <- streams %>% ungroup() %>% mutate(streams=ifelse(runif(n())<.3,NA, streams)) %>% arrange(artist,country,date)

# This snippet fills these observations, by carrying the "last observation (that's not NA) forward"
streams <- streams %>%
  group_by(artist, country) %>%
  mutate(streams_filled = na.locf(streams, na.rm = FALSE))
```

### Lags and Leads

Lags help compare values with previous observations, while leads look ahead.

#### Example: Compute daily change in streams

```{r}
streams <- streams %>%
  group_by(artist, country) %>%
  arrange(date) %>%
  mutate(
    lag_streams = lag(streams_filled),
    daily_change = streams_filled - lag_streams
  )

streams %>% arrange(artist,country,date) %>% head(10)
```

### Creating Counters

Sometimes we need to generate row numbers or observation counters.

#### Example: Assign an index to each artist's entries

```{r}
streams <- streams %>%
  group_by(artist, country) %>%
  mutate(entry_number = row_number())

streams %>% select(date, artist, country, entry_number) %>% arrange(artist, country, date) %>% head(10)
```

### Conversion with Dates

Date manipulation is critical for aggregating data by weeks, months, or years.

#### Example: Convert date to week and extract the year

```{r}
library(lubridate)

streams <- streams %>%
  mutate(
    week = isoweek(date),
    month = month(date),
    year = year(date)
  )

streams %>% select(date, week, month, year) %>% head(10)
```

### Week to Month & Month to Date Conversion

Aggregating data by different time frames helps in trend analysis.

#### Example: Aggregate streams by month

```{r}
monthly_streams <- streams %>%
  group_by(year, month, artist) %>%
  summarize(total_streams = sum(streams, na.rm = TRUE))

monthly_streams %>% head(10)
```

This concludes **Part 1** of our tutorial. In the next part, we will apply these techniques in real-world marketing case studies!



Here's the refined **Merging Data** section with a more balanced discussion on `full_join()`, plus an explanation of `merge()` from base R and when to use or avoid it.  

## Merging Data: Combining Multiple Data Sources  

In real-world analysis, data is often stored in separate tables. To get meaningful insights, we need to merge datasets based on shared keys. The `dplyr` package provides efficient functions for this, making it easy to combine and manipulate data.

### Understanding Join Types  

Joins allow us to merge two datasets by matching values in a **key column**. Here‚Äôs an overview of different join types:

![Common Join Types](https://raw.githubusercontent.com/rstudio/cheatsheets/main/joins.png)  

| Join Type  | Keeps All Rows From | Drops Rows? |
|------------|--------------------|-------------|
| **Left Join (`left_join()`)** | Left table | No |
| **Inner Join (`inner_join()`)** | Only matching rows | Yes |
| **Right Join (`right_join()`)** | Right table | No |
| **Full Join (`full_join()`)** | Both tables (fills missing values) | No |

The **left join is the default choice** in most cases because it keeps all rows from the left dataset and only adds matching values from the right. Inner joins should be used with caution because they remove rows that do not have a match.


### What Are Keys in a Join?  

To merge data correctly, we need **keys**‚Äîcolumns that uniquely identify records in each dataset.  

- **Single key**: Merging by a single column like `"date"`.  
- **Composite key**: Merging by multiple columns like `"date"` and `"artist"`.  

Without defining a proper key, the merge may not work as expected, or it may introduce duplicate rows.

### Examples of Joins in Action

#### Example: Left Join (Recommended)  

Merging `streams` with `playlists`, keeping all rows from `streams`.  

```{r}
# For this exercise, let's add a week column that we can merge on
streams <- streams %>% mutate(week=ISOweek::ISOweek(date))

merged_data <- streams %>%
  left_join(playlists, by = c("artist" = "artist", 
                              "week"="iso_week"))  # Keeps all stream data, adds playlist info.

# Note: here we specify that "week" in the LEFT data frame (streams) is matched on the "iso_week" column in the RIGHT data frame (playlists). 

head(merged_data)
```

Why use a left join?  
- All streaming data remains intact.  
- Playlist data is added where available, but missing values appear if no match is found.  
- This is useful when missing data in playlists should not remove streaming records.

#### Example: Inner Join (Only Matching Rows)  
Keeps only rows where a match exists in both `streams` and `socials`.  

```{r}
tiktok_followers = socials %>% filter(platform=='TikTok'  & metric == 'followers')

merged_data <- streams %>%
  inner_join(tiktok_followers, by = c("date", "artist"))  # Match by both columns

# rename value column
merged_data = merged_data %>% mutate(tiktok_followers = value) %>% select(-platform, -metric, -value)

head(merged_data)
```

Things to keep in mind:  
- Rows are removed if an artist has streams but no corresponding social media data on that date. 
- This join is only useful when seeking to analyze cases where **both datasets** must have relevant information.

Note: Before `dplyr`, R users relied on the `merge()` function from base R. It works similarly but is less readable and much slower. In other words: please use `tidyverse`/`dplyr`.

### Key Takeaways  

- **Use `left_join()` as the default** because it keeps all rows from the left dataset.  
- **Be cautious with `inner_join()`**, as it removes rows with no match.  
- **`merge()` is a base R alternative**, but `left_join()` is preferred for readability and efficiency.  

For more details, check out the official **dplyr cheatsheet** on joins:  
[Joins Cheatsheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/joins.png)  

## Pivoting Data: Reshaping from Long to Wide and Wide to Long  

Data is often stored in **different formats**, depending on the analysis needs. Sometimes, we need to **widen** a dataset to make it easier to read, and other times, we need to **lengthen** it to perform certain operations. The `tidyr` package in `tidyverse` provides **pivoting functions** to reshape data efficiently.  


### Wide vs. Long Format

- **Wide format**: Each row represents a single entity, with multiple columns for different variables.  
- **Long format**: Each observation is in its own row, with a single column for values and an **additional column indicating the variable type**.  

| Format | Example Structure | Example Dataset
|--------|------------------|--------------|
| **Wide** | One row per date, multiple columns for weather indicator by country | `weather.csv` |
| **Long** | One row per artist-date-country combination, a column for streams | `streams.csv` |


### From Wide to Long: `pivot_longer()`  

When converting **wide data** to **long format**, we gather multiple columns into key-value pairs. This is useful when variables are spread across columns but should be stored as rows.

Key arguments in `pivot_longer()`:  

1. **`cols`** ‚Äì Selects which columns to pivot.  
   - Can specify by name (`c(BE_temp, DE_temp)`) or range (`BE_temp:US_temp`).  
   - Use `-date` or `!date` to pivot everything except certain columns.  

2. **`names_to`** ‚Äì Defines where column names will be stored.  
   - Example: `"country"` if columns like `DE_temp` and `BE_temp` should be labeled by country.  

3. **`values_to`** ‚Äì Defines where the actual data values go.  
   - Example: `"temperature"` if numeric values from `DE_temp` and `BE_temp` represent temperature readings.  

This transformation makes it easier to filter, group, and analyze structured data.  

#### Example: Make `weather` data long by country
The `weather` dataset stores temperature data **by country** in long format. We want to convert it into wide format, with one row per `date` and separate columns for each country‚Äôs temperature.

```{r}
long_weather <- weather %>%
  pivot_longer(cols = ends_with('temp'), names_to = "name", values_to = "temperature") %>% separate(name, into = c('country','variable'), sep='_') %>% select(date, country, temperature)


head(long_weather)
```

**Before pivoting (wide format)**:

| date       | BE_temp | DE_temp |
|------------|--------|--------|
| 2024-01-01 | 11.6   | 14.3   |
| 2024-01-02 | 11.4   | 8.7    |

**After pivoting (long format)**:

| date       | country | temperature |
|------------|---------|-------------|
| 2024-01-01 | BE      | 11.6        |
| 2024-01-01 | DE      | 14.3        |
| 2024-01-02 | BE      | 11.4        |
| 2024-01-02 | DE      | 8.7         |

üìå **Why use `pivot_longer()`?**  

- Makes data **tidier** for filtering, grouping, and visualization.  
- Essential for **ggplot2** and **time-series analysis**, where values should be in a single column.


### From Long to Wide: `pivot_wider()`

When converting **long data** to **wide format**, we spread values across multiple columns, making the dataset easier to read and analyze.

Key arguments in `pivot_wider()`:  

1. **`id_cols`** ‚Äì Specifies the column(s) that should remain unchanged.  
   - Example: `"date"` if we want to keep dates as row identifiers while spreading other values.  
   - If not specified, `pivot_wider()` assumes all non-selected columns should be spread.  

2. **`names_from`** ‚Äì Defines which column contains the new column names.  
   - Example: `"country"` if values from `"country"` (`DE`, `BE`, `NL`) should become separate columns.  

3. **`values_from`** ‚Äì Specifies which column contains the data to be spread across new columns.  
   - Example: `"temperature"` if we want each country's temperature as its own column.  

This transformation is useful when preparing data for reports, modeling, or visualizations.  

#### Example: Make `long_weather` wide again

```{r}
wide_weather <- long_weather %>% 
  pivot_wider(names_from = 'country', values_from = 'temperature')

head(wide_weather)
```


**Before pivoting (long format)**:

| date       | country | temperature |
|------------|---------|-------------|
| 2024-01-01 | BE      | 11.6        |
| 2024-01-01 | DE      | 14.3        |
| 2024-01-02 | BE      | 11.4        |
| 2024-01-02 | DE      | 8.7         |

**After pivoting (wide format)**:

| date       | BE_temp | DE_temp |
|------------|--------|--------|
| 2024-01-01 | 11.6   | 14.3   |
| 2024-01-02 | 11.4   | 8.7    |


üìå Why use `pivot_wider()`?

- Makes the dataset **easier to read** when working with multiple categories.  
- Useful when preparing data for **visualizations or modeling**.

### Key Takeaways

- Use `pivot_wider()` when you need **one row per entity** with multiple value columns.  
- Use `pivot_longer()` when you need **one row per observation**, making it easier for analysis.  
- Pivoting is **reversible**‚Äîyou can always reshape data back if needed.  

For more details, check out the **tidyverse cheatsheet on pivoting**:  
[tidyr Pivoting Cheatsheet](https://tidyr.tidyverse.org/reference/pivot_longer.html)  


# Part 2: Case studies and Exercises




Exploring and auditing new data with RMarkdown (in-class tutorial)
========================================================
author: Hannes Datta
date: 
autosize: true


<style>
.small-code pre code {
  font-size: 1em;
}
</style>

Before we get started
=====================

- Group work will get started - ensure your teams are properly registered
  - Both on Canvas
  - ...and on GitHub: https://classroom.github.com/classrooms/107044217-course-dprep-classroom-spring-2025
  - please use same team names both on Canvas and GitHub (thanks!)
- Any pending installation issues? Get in touch with Roshini during the coaching session.

<!--
LINK TO GH to be updated.
old:https://classroom.github.com/a/K4FxS25n

https://classroom.github.com/classrooms/107044217-course-dprep-classroom-spring-2025
- Looking back...
  - coaching session: goal to get setup on GitHub
  - tutorial: way of working -- individual/team; issue-based
  - technical issues with Git? see, e.g., [password/authentication](https://dprep.hannesdatta.com/docs/course/support)
- Reality check: doubting whether to stay enrolled? Tell your team about how you feel!


+ show GPT and debugging [!!!]
--> 

Structure of this tutorial
========================================================

- Data preparation theory ("the bigger picture")

- Today's **tutorial**: we will use RMarkdown to start exploring new data

- After class
  - Work through [chapters 6-7](https://datacarpentry.org/r-socialsci/06-rmarkdown.html) of "R for Social Scientists" (on R Markdown & ggplot)
  - Complete exercises in [today's tutorial](https://dprep.hannesdatta.com/docs/modules/week2/tutorial/intro-to-r.html)
  - Optional: [Intermediate R on Datacamp](https://www.datacamp.com/courses/intermediate-r)

Data preparation theory (I) - Research workflow
====

![Main goals in each step of the research workflow](fig_researchworkflow.png)

Data preparation theory (II): Components of source code
====

1) Initializing the script/setup
- Loading packages
- Making connections to databases
- Any other "main" parameters (e.g., "Knowing" whether to prototype or not)
2) Input
- Read data (e.g., unstructured/unstructured data, remote/local locations, files or databases)
3) Transformation
- E.g., filtering, aggregation, merging, transformation, deduplication (more later)
4) Output
- Store (intermediate) data, figures (e.g., auditing, final)


Data preparation theory (III): A research pipeline
====

![Research pipeline](fig_pipeline.png)

Today's session
===

- Today, we're “just” exploring data
- ...but, some of the source code will end up in your data preparation for your project.

![Today's focus: Step 1 of research workflow](fig_researchworkflow.png)

Exploring data using RMarkdown
==============
incremental: true

- Recall, there are multiple ways to code in R
  - RStudio
  - Your favorite script editor (e.g., VS Code) + running from the terminal
- But -- in all of these cases, the output will be "ugly" (i.e., just some text)
- RMarkdown is a way to format output nicely -- and ideally suited for exploring data
  - mix of code and text (formatted in markdown) - you already know the concept from Jupyter Notebook
  - possibility to compile into HTML, PDF, Word, etc.
- great for prototyping code, but bad for "production" (this is also the case for Python's Jupter Notebook)

DO: Creating your first RMarkdown
=========

In today's session, we create our own RMarkdown document.

1. Select file --> new --> RMarkdown.
2. Choose HTML as output, and pick a good file name (e.g., "tutorial")
3. Try to __compile__ the document by clicking on "knit to HTML". Save your document before first!

![](fig_knit.png)

__We will continue when you're done.__

RMarkdown basics
======
incremental: true

- We already know markdown syntax, right?
  - If not: find a cheat sheet online right now!
- All compiled the document? Try this first __always__!
- Code cells & running code (click, ctrl/cmd+enter)
- Using inline code
- Options to run (or not run) code cells when compiling documents
  - that 'little wheel'
  - `echo`, `include`
  
  ![](fig_cell.png)


Let's get started with exploring and auditing data
=====

We'll make use of the __Google Mobility Dataset__

1. Run the code snippet below

```{r}
library(tidyverse)
mobility <- read_csv('https://raw.githubusercontent.com/hannesdatta/course-dprep/refs/heads/master/content/docs/modules/week2/tutorial/2020_NL_Region_Mobility_Report.csv')
```

2. Start exploring the data: use `head()`, `tail()`, and `View()`


Unit of analysis
===========

- What is it? → The level at which data is collected (e.g., individuals, groups, products, events).  
- Why does it matter? → It shapes **how you summarize and interpret data**—wrong levels lead to **misleading results**.  
- Example:  
  - A store tracks **purchases** → Unit = **transaction**  
  - To study **customer behavior**, we must **aggregate by customer** (or **customer-month**, etc.) 

<br>
- Key Questions:  
  - 👉 *What is my current unit of analysis?*  
  - 👉 *Is this the right level for my research question?*  
  - 👉 *Do I need to aggregate or transform my data?*  

Creating summary statistics
========
incremental: true

- We can generate summary statistics using the `summary()` command.
  - do values make sense?
  - are there any missing values?
- We can also zoom in on specific columns and create summary stats (e.g., on `retail_and_recreation_percent_change_from_baseline`) - what does it mean?


Cleaning the data
=========

- Typically, the raw data is always messy and likely not at the correct unit of analysis (e.g., city level).
- Also, we may still want to select columns, or rename them.

<br>
__DO:__

1. Drop these columns: `country_region_code`, `metro_area`, `iso_3166_2_code`, `census_fips_code`
2. Rename `sub_region_1` to `province`, and `sub_region_2` to `city`

Solution
====

```{r}
mobility <- mobility %>% rename(province = sub_region_1,
                                city = sub_region_2,
                                retail_and_recreation = retail_and_recreation_percent_change_from_baseline,
                                grocery_and_pharmacy = grocery_and_pharmacy_percent_change_from_baseline,
                                parks = parks_percent_change_from_baseline,
                                transit_stations = transit_stations_percent_change_from_baseline,
                                workplaces = workplaces_percent_change_from_baseline,
                                residential = residential_percent_change_from_baseline)
                
```


Verifying data types
=========

- `class(mobility$date)`
- Can only calculate with dates when they are actual dates!

Do: Selecting subsets
======

- Can you explore the data a bit more? Which observations are measured at the __country level__?
- How can you select only these country-level observations, using `%>% filter(...)`?
- Put the province level data sets into `province`, the country-level datasets into `country`

Solution
====

Please all run this snippet to be able to continue the tutorial if you have not solved it yourselves.

```{r}
country <- mobility %>% filter(is.na(city) & is.na(province))
province <- mobility %>% filter(is.na(city) & !is.na(province))
```

Missing values
=================

- Why care about missing values?  
  - Missing data can **bias results** and **affect analysis**  
  - Some methods ignore missing values, others break if not handled  
  - Solutions depend on **why** values are missing  
- Ways to explore missing values
  - `summary()`
  - `sum(is.na(columnname))`
  
DO: Explore missing values in the `country` dataset using `summary()`



DO: Replacing missing values
=================
 
- `mutate`: create new or edit existing column
- `ifelse(X, A, B)` does action A or B, depending on X
- `is.na(col)` checks for missing values in a column

Please replace missing values by their mean for `retail_and_recreation` and other "place" columns in the `province` dataset.

Solution
=====

```{r}
province_updated = province %>% mutate(retail_and_recreation = ifelse(is.na(retail_and_recreation), mean(retail_and_recreation,na.rm=T), retail_and_recreation),
                               grocery_and_pharmacy = ifelse(is.na(grocery_and_pharmacy), mean(grocery_and_pharmacy,na.rm=T), grocery_and_pharmacy),
                               parks = ifelse(is.na(parks), mean(parks,na.rm=T), parks),
                               transit_stations = ifelse(is.na(transit_stations), mean(transit_stations,na.rm=T), transit_stations),
                               workplaces = ifelse(is.na(workplaces), mean(workplaces,na.rm=T), workplaces),
                               residential = ifelse(is.na(residential), mean(residential,na.rm=T), transit_stations)
                               )

```

Visualization using `ggplot2`
===========

- Why?
  - great plotting tool: `ggplot2` (better than built-in functionality)
  - works seamlessly with `dplyr` pipelines
  - can create really complex charts, real quick, plus export them to PDF/PNG, etc.

- Steps
  - define dataset: `ggplot(data = your_data)` or `data %>% ggplot()`
  - set easthetics (aes): `aes(x= ..., y = ...)`
  - choose geometry/chart type (`geom_`): `geom_line()`, `geom_point`, `geom_col` (barchart)
  - customize (e.g., titles, colors, themes, labels, etc.)
  
DO: First chart
=====

```{r}

library(ggplot2)

ggplot(data = country) + 
  geom_line(aes(x = date, y = retail_and_recreation), 
    color = "black") +
  labs(title = "Changes in Visits to Retail Areas Over Time",
       x = "Date",
       y = "% Change from Baseline") +
  theme_minimal()
```

1. Change color of lines to blue
2. Try out another theme: `theme_light`
3. Change the y axis from `retail_and_recreation` to `parks` + change title


Adding more data points to a chart
=====

```{r}

library(ggplot2)

ggplot(data = country) + 
  geom_line(aes(x = date, y = retail_and_recreation, color = 'Retail')) +
  geom_line(aes(x = date, y = parks, color = 'Parks')) +
  scale_color_manual(values = c("Retail" = "red", "Parks" = "blue")) +
  labs(title = "Time spent at retail vs. parks",
       x = "Date",
       y = "% Change from Baseline") +
  theme_minimal()
```

DO: Can you also add information from time spent at work?


Grouping via facet wrap
=====

Sometimes, it makes sense "automatically" grouping our plots, such as by province!

```{r}
province_updated %>% ggplot() + 
  geom_line(aes(x = date, y = retail_and_recreation, color = 'Retail')) +
  geom_line(aes(x = date, y = parks, color = 'Parks')) +
  facet_wrap(~province) + 
  scale_color_manual(values = c("Retail" = "red", "Parks" = "blue")) +
  labs(title = "Time spent at retail vs. parks",
       x = "Date",
       y = "% Change from Baseline",
       color = "Place") +
  theme_minimal()
```

Grouping via "key" column in a long data set
=====

Run this!

```{r}
province_long <- province_updated %>%
  select(province,date, retail_and_recreation, grocery_and_pharmacy, parks, transit_stations, workplaces, residential) %>%
  pivot_longer(cols = c(retail_and_recreation, grocery_and_pharmacy, parks, 
                        transit_stations, workplaces, residential),
               names_to = "place_category",
               values_to = "mobility_change") 
```

Grouping via "key" column in a long data set
=====

```{r}
province_long %>% ggplot(aes(x=date, y=mobility_change, color=place_category)) +
  geom_line()  + 
  facet_wrap(~province)+
  theme_minimal()
```

Saving plots
====

```r
ggsave()
```

Wrapping everything up in a "clean" script
==============

1. Setup done/packages and other prerequisites loaded?
2. Adhered to input - transformation - output?
3. Clean up any remaining garbage
4. File callable from the command prompt? --> we will need this for automation in subsequent tutorials (`Rscript <filename>`)


Recap: Steps in data preparation
================

__Each and every source code file obeys to the "setup-ITO" procedure__.

- 1) Setup such as loading packages, setting variables, making database connection
- 2) Input (e.g., loading data)
- 3) Transformation
  - many options...: filtering, grouping and summarizing, creating new variables, etc.
  - mostly leads to a "different primary key than the input data" (--> unit of analysis!)
- 4) Output (e.g., saving, passing on to the next script)

Let's zoom out again
==============

Remember the workflow from the start of today's session?

![Main goals in each step of the research workflow](fig_researchworkflow.png)

We've just covered step 1. Concepts and programming knowledge extends to subsequent parts.

Next steps
===============

- Revisit RMarkdown & ggplot on Data Carpentry
- Work through the tutorial and complete exercises (all solutions available)
- Start working on team projects
  - pick a dataset: IMDb or Yelp
  - come up with a research question (but note: this course is about *engineering a research pipeline*, not about rigorously answering the RQ!)


Exploring and auditing new data with RMarkdown (in-class tutorial)
========================================================
author: Hannes Datta
date: 
autosize: true


<style>
.small-code pre code {
  font-size: 1em;
}
</style>

Before we get started
=====================

- Group work will get started - ensure your teams are properly registered
  - Both on Canvas
  - And on GitHub: https://classroom.github.com/a/miO8DP3Y
- Any pending installation issues? Let me know!
- Today's session is *interactive*: I brought signs to indicate how far you are
  - green: I'm done
  - yellow: working on it...
  - red + raise arm: need help!

<!--
- Looking back...
  - coaching session: goal to get setup on GitHub
  - tutorial: way of working -- individual/team; issue-based
  - technical issues with Git? see, e.g., [password/authentication](https://dprep.hannesdatta.com/docs/course/support)
- Reality check: doubting whether to stay enrolled? Tell your team about how you feel!
--> 

Structure of this tutorial
========================================================

- Data preparation theory ("the bigger picture")

- Today's **tutorial**: we will use RMarkdown to start exploring new data

- After class
  - Complete exercises in [today's tutorial](https://dprep.hannesdatta.com/docs/modules/week2/tutorial/intro-to-r.html)
  - Work through [chapter 7](https://datacarpentry.org/r-socialsci/06-rmarkdown.html) of "R for Social Scientists" (on R Markdown)
  - Optional: [Intermediate R on Datacamp](https://www.datacamp.com/courses/intermediate-r)

Data preparation theory (I) - Research workflow
====

![Main goals in each step of the research workflow](fig_researchworkflow.png)

Data preparation theory (II): Components of source code
====

1) Initializing the script/setup
- Loading packages
- Making connections to databases
- Any other "main" parameters (e.g., "Knowing" whether to prototype or not)
2) Input
- Read data (e.g., unstructured/unstructured data, remote/local locations, files or databases)
3) Transformation
- E.g., filtering, aggregation, merging, transformation, deduplication (more later)
4) Output
- Store (intermediate) data, figures (e.g., auditing, final)


Data preparation theory (III): A research pipeline
====

![Research pipeline](fig_pipeline.png)

Today's session
===

- Today, we're “just” exploring data
- ...but, some of the source code will end up in your data preparation for your project.

![Today's focus: Step 1 of research workflow](fig_researchworkflow.png)

Exploring data using RMarkdown
==============
incremental: true

- Recall, there are multiple ways to code in R
  - RStudio
  - Your favorite script editor (e.g., VS Code) + running from the terminal
- But -- in all of these cases, the output will be "ugly" (i.e., just some text)
- RMarkdown is a way to format output nicely -- and ideally suited for exploring data
  - mix of code and text (formatted in markdown) - you already know the concept from Jupyter Notebook
  - possibility to compile into HTML, PDF, Word, etc.
- great for prototyping code, but bad for "production" (this is also the case for Python's Jupter Notebook)

DO: Creating your first RMarkdown
=========

In today's session, we create our own RMarkdown document.

1. Select file --> new --> RMarkdown.
2. Choose HTML as output, and pick a good file name (e.g., "tutorial")
3. Try to __compile__ the document by clicking on "knit to HTML". Save your document before first!

![](fig_knit.png)

__We will continue when you're done.__

RMarkdown basics
======
incremental: true

- We already know markdown syntax, right?
  - If not: find a cheat sheet online right now!
- All compiled the document? Try this first __always__!
- Code cells & running code (click, ctrl/cmd+enter)
- Using inline code
- Options to run (or not run) code cells when compiling documents
  - that 'little wheel'
  - `echo`, `include`
  ![](fig_cell.png)

Reviewing basic programming concepts in Base R
==============
incremental: true

- Last week, we covered __basic__ programming concepts in "base R".
- We will first review some of these concepts.
- It's good to have a cheat sheet available - use [this](https://github.com/rstudio/cheatsheets/blob/main/base-r.pdf)

Do: Recap from our bootcamp
================

Let's create some data!

```{r}
set.seed(1234)
campaign_responses <- floor(sample(c(NA, rnorm(100, mean = 50, sd = 20)), 1E3, replace = TRUE))

```

<br>
__Write code for the following exercises:__

1. Find all campaigns with exactly 25 responses.
2. Identify campaigns with less than 25 responses (maybe we should adjust these campaigns?).
3. Count the campaigns that received more than 50 responses (these are our top performers).
4. Count how many campaigns have missing response data (to ensure data integrity).
5. List campaigns with more than 50 responses OR missing data (high performers or data issues).
6. Identify campaigns with responses between 10 and 50 (average performers we could potentially optimize).

Do: Controls
================
class: small-code

- __If statements__ allow you to conditionally execute some parts of your code.
- Please first download some data - __observe the n_max statement!__

```{r}
download.file("https://raw.githubusercontent.com/hannesdatta/course-dprep/master/content/docs/modules/week4/regional-global-daily-latest.csv", "streams.csv")
library(tidyverse)
streams <- read_csv('streams.csv', skip=1, n_max = Inf)
```

- __Imagine the data had millions of rows. Write some code that only opens the first 100 rows if a variable called `prototype` is `TRUE`. If `prototype` is `FALSE`, all rows need to be loaded.__

Solution
================
class: small-code

```{r}
download.file("https://raw.githubusercontent.com/hannesdatta/course-dprep/master/content/docs/modules/week4/regional-global-daily-latest.csv", "streams.csv")
library(tidyverse)

prototype = TRUE
n_max = Inf
if (prototype==TRUE) n_max = 100
streams <- read_csv('streams.csv', skip=1, n_max = n_max)
```

__Beware:__
- `read.csv` is NOT `read_csv` (the latter is more efficient!)
- n_max / nrows (for `read.csv`) are __critically important__!

Why are we doing this again?
=============

Remember our research workflow? Before we can start with exactly the same data, we all need to download this data set first.

__Question:__ Why not do it manually? What could go wrong?


Loops
=====
class: small-code

- Loops are super handy to execute functions over and over again.

```{r}
urls = c('title.episode.tsv.gz', 'title.ratings.tsv.gz')

for (url in urls) {
  url_to_download = paste0('https://datasets.imdbws.com/', url) # complete the URL
  filename = url # set filename where the data needs to be saved
  download.file(url_to_download, destfile = filename) # download file
}
```

__Do:__ Use the code snippet above to download __also__ the "ratings" data from IMDb. Check whether files have been saved properly!

Solution
=========

```
# your code here
# urls = # Assemble list of URLs here
# then, copy paste code snippet for downloading and renaming data

```

Loops versus the "apply family"
============
incremental: true

- Loops don't "return" anything - they just execute stuff over and over again
- Functions from the `apply` family, though, DO return information.
- Apply comes in multiple flavors:
  - `lapply` (loops over a `vector` or `list`, returns a `list`)
  - `sapply` (loops over a `vector`, returns as a `vector`)
  - `apply` (loops over rows or columns of a matrix, returns a `vector`)
  - Other versions of `apply` exist, but I rarely use them
- Applies are super handy when working with data

Using `lapply` for crunching data
===========
class: small-code

```{r}
urls = c('https://datasets.imdbws.com/title.episode.tsv.gz', 'https://datasets.imdbws.com/title.ratings.tsv.gz')

datasets <- lapply(urls, read_delim, delim='\t', na = '\\N')
```

__Do__ 

1. run the code snippet above, and inspect the data. You can enter each data set by typing `datasets[[1]]`, `datasets[[2]]`, etc. - does it correspond to what you would expect?
2. Obtain summary statistics for all data sets, using `lapply` and the `summary`. 

Why again `lapply`?
=========

__What's the key difference between the "apply family" and "for loops"?__

Solution
======

- `lapply` "returns" stuff; loops just execute stuff without returning anything

Some more programming concepts that help us transform data
=========
class: small-code

- Regular expressions are extremely powerful to work with text data -- movie titles
  - `grepl` can filter for information, without typing the "exact" search query
  - `gsub` helps you to replace information
  
```{r}
episodes = datasets[[1]] 
episodes %>% filter(grepl('tt003', tconst))
```

__DO__

1. Search for the episodes of "Breaking Bad" (`903747`) using `filter` and `grepl` (in `datasets[[1]]`)
2. Search for the rating "The Wolf of Wall Street" (`0993846`) using `filter` and `grepl` (in `datasets[[2]]`). 

Functions
==========
class: small-code

- Sometimes, we would like to re-use code. For this, we make use of self-made functions. 
- For example, a function that searches for different movies/titles in the IMDb data.
- Always start by prototyping your function.

```{r, results = 'hide'}
episodes = datasets[[1]]
episodes %>% filter(grepl('tt003', tconst))
```

__Starting code:__

```{r, results = 'hide'}
search <- function(searchterm) {
  # put some code here

}
```

Summary of concepts covered so far
=================

- setup-ito
- knitting markdown documents
- conditional logic
- controls
- downloading data
- loops and the `apply` family (`lapply`)
- functions

Now we can get started with exploring and auditing data
=====

- We'll make use of the __Google Mobility Dataset__

__DO:__

1. Download the __regional CSV__ from https://www.google.com/covid19/mobility/ & unzip
2. Load the data into R using `read_csv()`
3. Start exploring the data: use `head()`, `tail()`, and `View()`

Unit of analysis
===========

- __Unit of analysis__: Refers to the specific entity or level of observation at which you analyze data 

- __Why is it important?__ It determines how you aggregate, summarize, and interpret your data. Without knowing your unit of analysis -- at each part of your project -- you will likely get lost.

- __Example__: The episodes and rating data is organized along the individual *episode*-level. Using the column `parentTconst` in `datasets[[1]]`, you could aggregate this data to another unit of analysis: the series level.

Summary statistics
========
incremental: true

- We can generate summary statistics using the `summary()` command.
  - do values make sense?
  - are there any missing values?
- We can also zoom in on specific columns
  - let's start making sense of the `retail_and_recreation_percent_change_from_baseline` variable: what does it mean?
  - let's plot this variable over time!
- If our target unit of analysis is different from the unit of analysis in the current data set, we can also use aggregation.

Cleaning the data
=========

- Typically, the raw data is always messy and likely not at the correct unit of analysis (e.g., city level).
- Also, we may still want to select columns, or rename them.

<br>
__DO:__

1. Drop these columns: `cols_to_drop <- c('country_region_code', 'metro_area', 'iso_3166_2_code', 'census_fips_code')`
2. Rename `sub_region_1` to `province`, and `sub_region_2` to `city`

We can also use more advanced functions, e.g., to wipe the `percent_change...` from the column names.

Verifying data types
=========

- `class(mobility$date)`
- Can only calculate with dates when they are actual dates!

Do: Selecting subsets
======

- Can you explore the data a bit more? Which observations are measured at the __country level__?
- How can you select only these country-level observations, using `%>% filter(...)`?

Wrapping everything up in a "clean" script
==============

1. Setup done/packages and other prerequisites loaded?
2. Adhered to input - transformation - output?
3. File callable from the command prompt? --> we will need this for automation in subsequent tutorials (`Rscript <filename>`)


Recap: Steps in data preparation
================

__Each and every source code file obeys to the "setup-ITO" procedure__.

- 1) Setup such as loading packages, setting variables, making database connection
- 2) Input (e.g., loading data)
- 3) Transformation
  - many options...: filtering, grouping and summarizing, creating new variables, etc.
  - mostly leads to a "different primary key than the input data" (--> unit of analysis!)
- 4) Output (e.g., saving, passing on to the next script)

Let's zoom out again
==============

Remember the workflow from the start of today's session?

![Main goals in each step of the research workflow](fig_researchworkflow.png)

We've just covered step 1. Concepts and programming knowledge extends to subsequent parts.

Next steps
===============

- Work through the tutorial and complete exercises (all solutions available)
- Start working on team projects
  - pick a dataset: IMDb or Yelp
  - come up with a 'research question' (but note: this course is about *engineering a research pipeline*, not about rigorously answering the RQ!)
- Join the coaching session, following today's tutorial session


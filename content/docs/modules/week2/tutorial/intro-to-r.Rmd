---
title: "Data exploration in R (dPrep)"
author: "Practice (updated 10 September 2023)"
output: webexercises::webexercises_default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(webexercises)
```

*In this tutorial, we'll walk you through the steps of exploring a data set and obtaining an understanding about the setting in which data was collected. After completion of the tutorial, you'll be able to produce an RMarkdown document, which you can render as a PDF or HTML document (e.g., to show others, initiate a discussion about data, etc.).*

*We've chosen a data set used by [Sim et al. 2022](https://doi.org/10.1287/mksc.2021.1321), in which the authors estimate the effect that COVID-19 had on music consumption on Spotify!*

*Along the way, you'll practice the skills you have been equipped with in the Datacamp tutorials.*


--- 

### Prerequisites
* [Installing R and R Studio](http://tilburgsciencehub.com/setup/r/)
* [DataCamp Intermediate R](https://www.datacamp.com/courses/intermediate-r)
* [Getting started with RMarkdown](https://datacarpentry.org/r-socialsci/06-rmarkdown.html)

--- 

### Learning Objectives
- Explore data sets in R
  - Read data from text files into data frames in R
  - Create and handle various data types in R (e.g., vector, matrix, dataframe)
  - Change the content of data frames (e.g., create, drop, or rename columns)
  - Filter data in data frames (e.g., by index, logical expressions, missing records, etc.)
  - Apply basic programming concepts (if-else statements, for-loop, functions)
- Assess data quality and obtain an understanding about the data
  - Calculate with data (e.g., taking an average)
  - Generate summary statistics (e.g., using `summary()` and `table()`)
  - Generate plots (e.g., using `plot()`)
  - Produce RMarkdown documents (HTML, PDF)

--- 

### Support Needed? 
For technical issues outside of scheduled classes, please check the [support section](https://dprep.hannesdatta.com/docs/course/support) on the course website.


---

### 1. Loading and Inspecting the Data

[Google’s COVID-19 Community Mobility Reports](https://www.google.com/covid19/mobility/) are used extensively by governments and researchers to get an understanding about the pandemic's effect on social life and the economy. In the data, Google provides information on *daily percentage changes* in the number of visits by place category, compared with the corresponding day of the week during the 5-week (pre-Covid) period between January 3 and February 6, 2020. The data consists of many observations that cover six categories of places: retail & recreation, grocery & pharmacy, parks, transit stations, workplaces, and residence. 

With the help of `dplyr`'s `read_csv()` function, we can now import the mobility data for the Netherlands into R. Download the [__Region CSVs__ archive from Google](https://www.google.com/covid19/mobility/) (you have to unzip it first!), create a *new R script* in the same directory, and read in `2020_NL_Region_Mobility_Report.csv`.

Tip: If R doesn't find your data, set your working directory properly (we typically click on SESSION --> SET WORKING DIRECTORY --> TO SOURCE FILE LOCATION). Hardcoding directories in your code (e.g., with `setwd()`) is not good practice.

```{r load_data}
mobility <- read_csv("2020_NL_Region_Mobility_Report.csv")

# Alternatively, you can use the data set stored on the course website.
# mobility <- read_csv('https://media.githubusercontent.com/media/hannesdatta/course-dprep/master/content/docs/modules/week3/tutorial/2020_NL_Region_Mobility_Report.csv', sep = ',')
```
\
**Exercise 1**     
Inspect the `mobility` data frame, and describe the *structure of the data set* in your own words. 
- Which column names does the data have?
- How are variables/metrics operationalized? 
- What is the unit of analysis at which the data is provided ("what uniquely identifies a row in this dataset?"). 
- How is the data sorted (if at all)?
- Are the values contained in each column complete? Do these values all make sense? 
- Are “strings” properly encoded (i.e., do they contain any weird characters, or do they seem appropriately rendered so you can read them)?

Tips: 

- Take a __look__ at the data using the function `head(mobility)`, or `View(mobility)`. If you'd like to view more rows with `head`, use `head(mobility, 100)` (for, e.g., the first 100 rows). (`tail()` gives you the last rows of the data).

- The command `summary(mobility)` generates descriptive statistics for all variables in the data. You can also use this command on individual columns (e.g., `summary(mobility$retail_and_recreation_percent_change_from_baseline)`).

- Character or factor columns are best inspected using the `table()` command.

\


```{r, webex.hide = 'Solution Exercise 1'}
head(mobility)

summary(mobility)

table(mobility$sub_region_1)

# The dataset shows the mobility changes at a national, regional, and city level in the Netherlands, for multiple categories of places. The records are sorted by date, and are grouped by location (starting with country stats).

# Moreover, three of the columns (`metro_area`, `iso_3166_2_code`, `census_flips_code`) contain only empty data, and two other columns (`sub_region_1`, `sub_region_2`) are blank for a subset of the values. This also holds for the columns related to percentage changes in mobility scores which are missing in some cases (especially on a city-level).
```

\

**Exercise 2**     
Suppose you'd like to extract data for the Netherlands as a whole (i.e., NOT at the regional or city level), how would you have to filter for it? Re-generate a data summary (`summary()`) on this new table, and inspect missing values.

```{r, echo = TRUE, webex.hide = 'Solution Exercise 2'}
filtered_df <- mobility %>% filter(is.na(sub_region_1) & is.na(sub_region_2))
summary(filtered_df)
```

The number of missing values on most variables has drastically decreased. That's good news! It seems like the data is most "complete" at the national level.

\
**Exercise 3**  
Let's now start zooming in on some of the metrics in the data to check what they really mean.

The data generally shows the percentage changes in visits to categorized places, compared to a normal value for that day of the week (i.e., baseline). The baseline day is the median value between January 3 and February 6, 2020. In other words, all reported numerical values are *relative to the first few weeks of 2020*. 

First, try to understand the "range" of the values. For example, percentages are commonly scaled between 0 and 1 (e.g., such that .50 means +50%), but sometimes also between 0 and 100 (e.g., such that 50 means 50%). What applies here?

Second, think of one or more problems that can arise as a result of how percentage changes were calculated (i.e., the choice for the baseline between January 3 and February 6, 2020). Come up with specific examples that illustrate problems!

```{r, webex.hide = 'Solution Exercise 3'}
### Part 1 ###

# Exploring the range
summary(mobility$retail_and_recreation_percent_change_from_baseline)
# --> the range seems to be measured on a scale on which 100 means 100%.


### Part 2 ###
# Problems that could arise from the baseline choice:

# - The data does not account for seasonality (e.g., park visits increase as the weather improves)  

# - In the Netherlands, we experienced a relatively mild winter period throughout 2020 (e.g., the average temperature in January 2020 was the 3rd highest ever). As a result, the time distribution of certain categories (e.g., parks, residence) may not be representative for a typical year. 

# - Probably a lot of other reasons! Try to think about more!
```

\

---

### 2. Data Cleaning & Transformation

Next, let's narrow down the data to a smaller set of data that we would like to concentrate on. For example, the raw data may contain columns we don't need or want, or specific subsets that we want to get rid of.

\

#### Drop Columns
To make working with the data frame more manageable, we drop columns we don't need. More specifically, we overwrite the dataframe with a selection of necessary columns (e.g., `df = df[,c("col_1", "col3", "col"4", "col5", ...)])`).

\
**Exercise 4**  
Drop the columns `country_region_code`, `metro_area`, `iso_3166_2_code` and `census_fips_code` from the `mobility` dataframe. How many columns do you end up with?


```{r, echo = TRUE, webex.hide = 'Bad solution', results = 'hide', warning = FALSE}
# bad solution - think about WHY?
mobility[, c(2:4, 8:15)]
```

```{r, echo = TRUE, webex.hide = 'Good Solution Exercise 4', message = FALSE}
# good solution
cols_to_drop <- c('country_region_code', 'metro_area', 'iso_3166_2_code', 'census_fips_code')
mobility <- mobility %>% select(-cols_to_drop)

head(mobility)

# how many columns do we retain?
ncol(mobility)
```

```{r eval=FALSE, warning=FALSE, include=FALSE, results='hide', webex.hide='Very suboptimal solution'}
# Very suboptimal solution
mobility[c(1,5,6,7)]<-list(NULL)
# The problem with this solution is in changing the columns. If a column changes, the numbers of the corresponding columns might also change, resulting in a different outcome of the code above.
```
\

#### Rename Columns
We continue with the (updated) data frame `mobility`.

As you can see below, some of the remaining column names of `mobility` are rather long (or can be described more precisely). 

```{r column_names}
# current column names
colnames(mobility)
```

You can rename a column of a data frame (`df`) by overwriting the current column names with a vector of new column names, like this: `colnames(df) = c("new_col_name_1", "new_col_name_2", "new_col_name_3", ...)` 

\

**Exercise 5**  
Trim the long column names (e.g., `retail_and_recreation_percent_change_from_baseline` becomes `retail_and_recreation`) and replace `sub_region_1` and `sub_region_2` by `province` and `city`, respectively. 

```{r, webex.hide = 'Bad solution', eval = FALSE}
# This solution works but is suboptimal, as the *format* of the input data may change (and hence, render the renamed columns wrong).
colnames(mobility) = c("country_region", 
                      "province",
                      "city", 
                      "place_id",
                      "date", 
                      "retail_and_recreation",
                      "grocery_and_pharmacy", 
                      "parks", 
                      "transit_stations", 
                      "workplaces", 
                      "residential")

```

```{r, webex.hide = 'Better solution'}
# First use the rename function to rename sub_region_1 & sub_region_2. After that, use the rename_with function to remove all the suffixes with "_percent_change_from_baseline'
mobility_updated <- mobility %>% rename(province = sub_region_1,
                                city = sub_region_2) %>%
                         rename_with(~str_remove(., '_percent_change_from_baseline'))
```

\

#### Convert into Date

Although the dates may look like "regular" dates (e.g., `2020-02-15`), R sometimes treats them like a character string (try running `class(mobility$date)` for yourself!). 

When working with `dplyr`/`tidyverse`, though, dates are mostly imported correctly.

If they are still characters rather than dates, you need to convert the `date` column into date format. Date conversion can cause you a mild headache, and it even gets more complex with varying date formats across different geographic regions. 

This is how to convert a character-encoded date to a "real date":

```{r}
mobility$date <- as.Date(mobility$date)
```

\

**Exercise 6**  
Now let's start calculating with our dates.

What's the first and last date in our data frame? Tip: you can use `min()` and `max()` now that the date column has been converted into date format! 

`r hide("Solution Exercise 6")`
The data runs from February 15, 2020 (`min(mobility$date)`) to December 31, 2020 (`max(mobility$date)`).

Observe above that you can not only use R code in code cells, but also in text by enclosing it in special characters: `r min(mobility$date)`.
`r unhide()`

\

#### Adding a New Column

Let's start generating some "derived metrics" - i.e., metrics that are based on the raw data, but that were not part of the actual raw data. Think of this as a first attempt to operationalize some variables that you're interested in.

For example, to get a proxy for overall movement trends, we could add `avg_mobility` to the data, which we define as the (row) mean of `retail_and recreation`, `grocery_and_pharmacy`, `parks`, `transit_stations`, `workplaces`, and `residential`. 

**Exercise 7**  
Add the `avg_mobility` column to your dataset (keep in mind that the data frame may contain one or more missing values). You may want to look up the documentation of the `na.rm` parameter. 

```{r, webex.hide = 'Solution Exercise 7', eval = FALSE}
# solution (most elegant and sustainable)

columns <- c('retail_and_recreation', 'grocery_and_pharmacy', 'parks', 'transit_stations', 'workplaces', 'residential')
mobility_updated <- mobility_updated %>% mutate(avg_mobility2 = rowMeans(select(., all_of(columns)), na.rm =TRUE))


```

```{r, webex.hide = 'Bad solution Exercise 7', eval = FALSE}
# prone to errors of referencing to the correct columns
avg_mobility_array <- mobility[c(6,7,8,9,10,11)]
# this is inefficient, too, because it does not make use of the dplyr functions (and hence, is considerably slower on large datasets)
mobility$avg_mobility <- rowMeans(avg_mobility_array, na.rm = TRUE)

# The answer above, same as for exercise 4 makes use of counting which is prone to error if we add or remove a column!

```

```{r eval=FALSE, webex.hide = 'Inefficient solutions'}
# solution (also not sustainable as column names could change!!!)
mobility$avg_mobility <- rowMeans(mobility[,6:ncol(mobility)], na.rm = TRUE)

# solution (least elegant - and far less efficient!!!) - do you notice the difference in running time?  

for(counter in 1:nrow(mobility)){
  row = mobility[counter, ]
  mobility[counter, 'avg_mobility'] = mean(c(row$retail_and_recreation,
                                             row$grocery_and_pharmacy, 
                                             row$parks, 
                                             row$transit_stations, 
                                             row$workplaces, 
                                             row$residential), 
                                           na.rm=TRUE)
}

```

\

#### Selecting a Subset of Data

At this point, we've realized the data contains information at the country, province and city level. We'd ideally like to split these data in three separate data sets. 

For this, we need to select a subset of the data, and store it in new data frames. 

We know that the values in the `province` and `city` columns are empty for the country level stats. In the same way, the city level records are blank for the data aggregated on the province level. This is shown in the table below where each `X` denotes the availability of data in a respective column.


| Aggregation Level | country_region | province | city | 
| :----- | :----- | :----- | :----- | 
| Country | X |  | | 
| Province | X | X | | 
| City | X | X | X | 

\

**Exercise 8**  
Create three subsets (`country`, `province`, `city`) of `mobility` according to the descriptions above. To filter for NA strings (`NA`) you can use the boolean expression `is.na(df$column)`. This essentially checks whether the column contains missing values.


```{r, webex.hide = 'Solution Exercise 8'}
country <- mobility_updated %>% filter(is.na(city) & is.na(province))
province <- mobility_updated %>% filter(is.na(city) & !is.na(province))
city <- mobility_updated %>% filter(!is.na(city))
```

\

**Exercise 9**  
Write a function `inspect_data()` that takes in a dataframe (`df`), and produces descriptive stats (`summary`), reports on the number of observations (`nrow()`) and columns (`ncol()`), and the range of dates in the data.

Start from the code snippet below:

```
inspect_data <- function(df) {
  cat('Generating some descriptive statistics...\n\n')
  cat('Data: ') # using cat, you can write text to the console/screen; "\n" is a new line.
  cat(deparse(substitute(df))) # this one prints the NAME of the input data set
  cat('\n\n')
  
  print(summary(df))
  # more here...
}
```

```{r eval=TRUE, webex.hide = 'Solution Exercise 9'}
# inspect_data
inspect_data <- function(df){
  cat('Generating some descriptive statistics...\n\n')
  cat('Data: ')
  cat(deparse(substitute(df)))
  cat('\n\n')
  
  cat('Summary statistics\n')
  print(summary(df))
  cat('\n\n')
  
  cat('Number of columns: ')
  cat(ncol(df))
  cat('\n\n')
  
  cat('Number of observations: ')
  cat(nrow(df))
  cat('\n\n')
  
  cat('Range of the data:\n')
  summary(df$date)
  cat('\n\n')

}

inspect_data(country)
inspect_data(province)
inspect_data(city)

```

Observe the high occurrence of missing levels at lower aggregation level (e.g., cities; especially parks). Further inspection of the [documentation](https://support.google.com/covid19-mobility/answer/9825414?hl=en&ref_topic=9822927) tells us that these data gaps are intentional because the data doesn't meet the quality and privacy threshold. 

**Exercise 10**

Let's zoom in on these missing observations, and create a table that shows the percentage missings, per data set and each of the columns below.

| subset | retail_and_recreation | grocery_and_pharmacy | parks | transit_stations | workplaces | residential | 
| :--- |:--- |:--- |:--- |:--- |:--- |:--- |
| `country` | 0 | 0 | 0 | 0 | 0 | 0 | 
| `province` | 0.49 | 0.08 | 7.34 | 1.02 | 0.23 | 0 | 
| `city` | 44.32 | 37.65| 82.40 | 45.11 | 6.43 | 45.39 |

Specifically, run the code below, and make it "run" on all data sets (country, province, city).


```{r}
# Code snippet
missings <- function(df, cols) {
  sapply(cols, function(x) length(which(is.na(df[, x])))/nrow(df))
}

columns <- c('retail_and_recreation', 'grocery_and_pharmacy', 'parks', 'transit_stations', 'workplaces', 'residential')

# Another way to define columns is as follows using the 'grep' function:
# columns <- grep('retail|grocery|park|transit|workplace|residential', colnames(mobility),value=T)

missings(country, columns)
```


```{r, webex.hide = 'Solution Exercise 10'}
result <- lapply(list(country, province, city), missings, cols = columns)

# we can also nicely "bind" them together
bind_rows(result)

#... and add data set names
data.frame(dataset = c('country','province','city'), bind_rows(result))
```

\

### 3. Data Exploration

\

Now that the data is in the right shape, it's finally time to visualize it, e.g., by using the built-in `plot()` function. If you're really into plotting, also check out the amazing package [`ggplot2`](http://ggplot2.tidyverse.org).

The basic `plot()` function takes the following input parameters:   

- `x` = the horizontal values  
- `y` = vertical values  
- `col` = color of the fill  (optional)
- `type` = `l` is a line chart, `p` is a scatter plot (optional)  

Here's an example of a time-series plot of the number of visits to parks: 

```{r}
plot(x = country$date, # horizontal
     y = country$parks, # vertical
     col = "green", # color
     type = "l") # line chart 
```

And here's another one that shows how to plot two lines in one graph:

```{r}
plot(x = country$date, # horizontal
     y = country$residential, # vertical
     col = "red", # color of chart
     type = "l",  # line chart
     ylim = c(-80, 30), # range of vertical axis
     ylab = "% change from baseline", # label of vertical axis
     xlab = "", # no label of horizontal axis
     main = "Less time at work partially compensated by more time at home") # plot title

lines(country$date, country$workplaces, col="blue") # add a second line chart

legend("bottomleft", c("residential", "workplace"), fill=c("red", "blue")) # legend (location, names, colors)
```

**Exercise 11**

Replicate the the following plot with R code (consider the labels, axes, colors, etc.):

```{r echo=FALSE}
plot(x = country[country$date > "2020-09-01", ]$date, 
     y = country[country$date > "2020-09-01", ]$transit_stations,
     col = "black", 
     type = "l",  
     ylim = c(-85, 0), # range of vertical axis
     ylab = "% change from baseline (January 2020)", 
     xlab = "", 
     main = "Transits to stations are down 75%") 
```
\

```{r, webex.hide = 'Solution Exercise 11'}
plot(x = country[country$date > "2020-09-01",]$date, 
     y = country[country$date > "2020-09-01",]$transit_stations, 
     col = "black", 
     type = "l",  
     ylim = c(-85, 0), # range of vertical axis
     ylab = "% change from baseline (January 2020)", 
     xlab = "", 
     main = "Transits to stations are down 75%") 
```

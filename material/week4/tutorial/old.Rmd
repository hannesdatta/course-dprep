---
title: "Untitled"
author: "Hannes Datta"
date: "2025-02-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Part 2: Merging 
In this section, we want to prepare the final dataset by merging the three datasets. The "join" functions from `dplyr` come in handy for this. 

## ðŸ“Œ Exercise 2: 

Examine the effect of picking a `left_join`, `right_join`, and `inner_join`, and how that affects the completeness of your data. Strive for a final data frame that captures as much of the original data as possible.

1. Merge the streams data with the songs dataframe. 
2. Merge the streams data with country_codes dataframe
3. Export the final data frame as `merged_df.csv` after merging all 3 datasets.

__Tips:__

- Identify common columns across the datasets.
- Look up the Datacamp tutorials for the syntax for join functions. 
- To avoid confusion between overlapping column names (e.g., country appears in both `streams.csv` and `country_codes.csv`), you may consider adding suffixes. 


```{r, webex.hide = 'Solutions to Exercise 2'}
df <- streams %>% 
  inner_join(songs, by = "song_id") %>%
  left_join(country_codes, by = c("country" = "country_code_2_letter"), suffix = c("_streams", "_cc"))

# Since countries in Oceania are missing in country_codes.csv we use a left join here (otherwise you'd miss out on +/- 139K records)

write_csv(df, "merged_df.csv")

```
# Part 3: Data Pre-Processing

## ðŸ“Œ Exercise 3a: Removing Duplicates
A track can appear in multiple top 200 lists across countries (e.g., both in the Netherlands and in Belgium) for sustained periods of time (e.g., yesterday and today). For that reason, a `song_id` may occur repeatedly within the data frame. Yet it can also happen that exactly the same record appears more than once. For example, the scraper that collected the data in the first place might have run twice which duplicated some of the records, or we accidentally made a copy-paste mistake ourselves. Check whether such duplicates are present in `df` and if so remove them. Export the result as `df_no_duplicates.csv`.

```{r, webex.hide = 'Solutions to Exercise 3a'}
#input
df <- read_csv("merged_df.csv")

# transformation
df <- df %>% filter(!duplicated(df))

# output
write_csv(df, "df_no_duplicates.csv")
```

## ðŸ“Œ Exercise 3b: Data Imputation

In roughly half of the cases, the `popularity` column is missing which turns out to be troublesome once you move towards a model building phase. Therefore, we ask you to impute these missing values by the median popularity score of the country on a given date. Here's a simple example to illustrate: 

| song_name | country | date | popularity | popularity_filled | 
| :---- | :---- | :---- | :----- | :---- |
| A | United Kingdom | 01-01-2020 | 91 | 91 |
| B | United Kingdom | 01-01-2020 | 66 | 66 |
| C | United Kingdom | 01-01-2020 | `NA` |  **66** |
| D | United Kingdom | 01-01-2020 | 62 | 62 | 
| E | United Kingdom | 02-01-2020 | 79 | 79 | 
| A | Netherlands | 01-01-2020 | 80 | 80 |

The `popularity_filled` column has been added which takes on the value of the `popularity` column if present and otherwise missing data (`NA`) are replaced with their imputed values. In this example, the popularity of song `C` in the United Kingdom has been computed as the median of `{62, 66, 91} = 66`. Note that it does not take into account song `E` in the United Kingdom (different date) and song `A` in the Netherlands (different country).   

Implement this data imputation strategy using the `filter`, `groupby`, and `summarise` functions from the `dplyr` package and the result as `df_imputed.csv`. We recommend first computing the imputation values (for each country across the days), and then replacing missing values in the `popularity` column with their matching counterparts. Evaluate your results in terms of the pros and cons of this approach and try to come up with an alternative imputation strategy (incl. arguments why you prefer that approach).

```{r, webex.hide = 'Solutions to Exercise 3b'}
# input
df <- read_csv("df_no_duplicates.csv")

# transformation
imputation_values <- df %>% filter(!is.na(popularity)) %>% 
  group_by(country, date) %>% 
  summarise(popularity_filled = median(popularity))

df_joined <- df %>% left_join(imputation_values, by = c("country" = "country", "date" = "date"))

df <- df_joined %>% mutate(popularity_filled = ifelse(is.na(popularity), popularity_filled, popularity))

# The main disadvantage is that missing records will always be assigned a median popularity score (so never abnormally low or high). Given the sheer amount of missing records, this would highly distort the distribution of the scores.
# Since the popularity score is country independent and it is either present for all records or not available for any country (or time period), we should find a way to group songs together. For example, if the artist has other songs for which the popularity score is known we could take the median of those scores, or we could look for songs with similar genre tag words.

# output
write_csv(df, "df_imputed.csv")
```
# Part 4: Feature Engineering

## ðŸ“Œ Exercise 4a: Creating Trend Variable

More likely than not, you have come across the `lm()` (i.e., linear model) function once before during your studies. Then, you also know that a regression model can only interpret numeric data. Categorical fields are typically converted into so-called dummy variables that take on either the value `0` or `1`. In a similar vein, dates are often converted into trend variables as input for a time-series model. For example, in our dataset January the 1st, of 2020 becomes 1 and the 15th of December 2020 (the last entry of `df`) becomes 350. Follow the steps below to create this trend variable for the `date` column of `df`: 

1. Study the [documentation](https://stat.ethz.ch/R-manual/R-devel/library/base/html/seq.Date.html) of the `seq` function to create a sequence of dates (see examples at the bottom of the page).
2. Create a data frame `dates` with two columns: `date` (i.e., the sequence of dates) and `counter` (i.e., a numeric value between 1 and 350). Tip: you may first need to convert the `dates` column into character format (`as.charcter(date)`).
3. Merge `df` and `dates` and inspect your results (which type of join do you use?)


```{r eval=FALSE, webex.hide = 'Solution Exercise 5'}
# Step 1
date <- seq(min(as.Date(df$date)), max(as.Date(df$date)), by = "day")

# Step 2
counter <- 1:length(date)
dates <- data.frame(counter=counter, date=date)

# Step 3
df <- df %>% inner_join(dates, by = "date")
```

## ðŸ“Œ Exercise 4b: Creating ranks 

Even though the data frame represents the Spotify top 200 charts, none of the columns indicates the rank of a given track. We can easily derive it from the number of streams, the `date`, and the `country` column. The built-in R function `ranks` returns a vector of ranks for a given input. By default, the lowest digit gets the highest rank (e.g., 1 becomes rank 1), you can reverse the order by adding a minus sign (`-`) in front of the input vector:

```{r}
rank_example = c(4, 9, 3, 2, 1)
# regular 
print(rank(rank_example))
# reversed
print(rank(-rank_example)) 
```

Add a `ranking` column to `df` using the `dplyr` package and export the result as `df_ranking.csv`. Although it is unlikely that some tracks have exactly the same number of streams, think about how you will deal with these edge cases (tip: run `?rank` to look up the documentation). Does your ranking always run from 1 to 200? Why is that? Investigate the root cause and fix it! 

```{r eval=FALSE, webex.hide = 'Solution Exercise 4b'}
# input
df <- read_csv("df_imputed.csv")

# transformation
df <- df %>% group_by(country, date) %>%
  mutate(ranking = rank(-streams, ties.method = "random"))

# The maximum ranking is 400 (rather than 200)
# Once we inspect one of the listings of df[df$ranking >= 200, ] we find that these records typically have redundant data somewhere hidden in between. For example, for df[(df$date == "2020-05-17") & (df$country == "HK"),] we discover that ranking 59 appears 17 times (!) which can be attributed to missing values `song_id` 

# remove rows without song_id (if you then recreate the ranking column you'll come to the conclusion the maximum ranking has become 200!)
df <- df %>% filter(!is.na(song_id))

# output
write_csv(df, "df_ranking.csv")
```
# Part 5: Pivoting the Data 

Most tracks are assigned to more than one genre. For example, the famous song "SeÃ±orita" by Shawn Mendes and Camilo Cabello is categorized as `pop`, but also as `canadian pop`, `post-teen pop`, `viral pop`, and `dance pop`. The data frame stores these records as a list of comma-separated genres. On a high level, we like to move from this state: 

| song_name | genres | 
| :----- | :---- | 
| SeÃ±orita | `pop, canadian pop, post-teen pop, viral pop, dance pop` | 


To a format in which the columns represent genre tag words and the table values indicate which genre(s) belong to which `song_name`: 

| song_name | k-pop | pop | canadian pop | post-teen pop | viral pop | dance pop | 
| :----- | :---- | :---- | :---- | :---- | :---- | :---- | 
| SeÃ±orita | 0 | 1 | 1 | 1 | 1 | 1 | 1 |
| Memories | 0 | 1 | 0 | 0 | 0 | 0 | 0 |
| Psycho | 1 | 0 | 0 | 0 | 0 | 0 | 0 |



To this end, we first need to separate the `genres` column of `df` into individual genres. We can use the built-in R function `strsplit` for that in the following way (note the double brackets `[[]]` around `1`!): 



```{r}
genres_example <- c("pop", "canadian pop", "post-teen pop", "viral pop", "dance pop")
genres_example_split <- split(genres_example, ",")
genres_example_split[[1]]

```

After some data wrangling we can transform the data into the following format: 

| song_id | song_genre | 
| :----- | :---- | 
| 13594414 | pop | 
| 13594414 | canadian pop | 
| 13594414 | post-teen pop | 
| 13594414 | viral pop | 
| 13594414 | dance pop | 
| 13977556 | pop` | 


## ðŸ“Œ Exercise 5: Reshape the data
As a last step, we use the `pivot_longer()` and `pivot_wider()` functions to change the format of our data frame (see, e.g., [this overview](https://tidyr.tidyverse.org/reference/pivot_longer.html) or these [code snippets](https://dcl-wrangle.stanford.edu/pivot-advanced.html)). We then store the results in `df_reshaped.csv`. 

Following these steps, it's your turn to create a data set, with (a) rows being the dates, (b) columns being the song IDs, and (c) the cells indicating a song's sum of streams.


```{r eval=FALSE, webex.hide = 'Solution Exercise 5'}
# load input
df <- read_csv("df_ranking.csv")

song_popularity = df %>% pivot_wider(id_cols = date, names_from = 'song_id', values_from = 'streams', values_fn = sum)

# let us look at the columns (--> these are song IDs!)
colnames(song_popularity)

# let us look at the rows (--> these are dates!)
song_popularity$date[1:10]

# output
write_csv(song_popularity, "df_reshaped.csv")
```

# Conclusion

Congratulations! ðŸŽ‰ You have successfully navigated the key steps of data preparation and gained hands-on experience with essential data wrangling techniques in R.  Here are the key takeaways from this tutorial:  

**âœ… Data Inspection & Cleaning**  

- Loaded and explored raw datasets to understand their structure.
- Identified missing values, out-of-range values, and encoding issues.
- Detected and removed duplicate records to ensure data integrity.


**âœ… Data Merging & Joins**

- Used inner_join(), left_join(), and right_join() to combine datasets and learnt how different join types affect data completeness.
- Resolved column conflicts by adding suffixes during merging.
  
**âœ… Handling Missing Values & Data Imputation**

- Applied imputation techniques to replace missing values using median-based imputation.
- Discussed pros and cons of different imputation methods and their impact on analysis.

**âœ… Feature Engineering & Transformations**

- Created new variables like trend indicators for time-series modeling and ranking.
- Filtered and processed text-based columns, such as genre extraction.

**âœ… Reshaping Data for Analysis**

- Used pivot_longer() and pivot_wider() to restructure datasets.

Great job completing this tutorial! Keep practicing! ðŸŽ‰  

## Support Needed? 

For technical issues outside of scheduled classes, please check the [support section](https://dprep.hannesdatta.com/docs/course/support) on the course website.








## Prerequisites

Ready to start? Here's what we recommend you do before going through this tutorial.

* [Datacamp Introduction to Tidyverse](https://datacamp.com/courses/introduction-to-the-tidyverse/data-wrangling-1?ex=1) (chapter 1 and 3)
* [Datacamp Cleaning Data in R](https://datacamp.com/courses/cleaning-data-in-r) (chapter 1 and 2)
* [Datacamp Joining Data with dplyr](https://datacamp.com/courses/joining-data-with-dplyr/joining-tables-1) (chapter 1 and 2)
## Extracting information from objects


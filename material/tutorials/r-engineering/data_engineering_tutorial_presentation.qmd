---
title: "Tutorial Data Engineering in R"
autor: "Javier Torralba" 
format: 
  beamer:
    theme: metropolis
    colortheme: seahorse
    linkcolor: blue
editor: visual
---
# Today's tutorial
- Back to R but maintain working with Git/GitHub. 
  - Part 1: expanding our knowledge of R
      - applying functions, advanced cleaning, dealing with time, merging data, pivoting
  - Part 2: selection of tutorial exercises. 

# Goals for today
- What we've covered earlier in this course
  - Basic wrangling/transformations (can you name a few examples?)
  - Structure scripts in three blocks -- required to automate workflow later (input, transformation, output)
- __Now:__ zoom in on more advanced ways to transform data

# Today's data:
- Massive, "big" data on music streaming
  - `streams.csv`: artist streams, daily
  - `weather.csv`: weather data for four countries
  - `socials.csv`: social media stats
  - `playlists.csv`: playlist statistics

# Download data

-   Go to this week's tutorial and copy the following:

```{r eval=FALSE, echo=TRUE}
if (!file.exists('downloaded_data.zip')) {
  download.file('https://github.com/hannesdatta/course-dprep/raw/refs/heads/main/material/tutorials/r-engineering/week4_data.zip', 
                'downloaded_data.zip')
  unzip('downloaded_data.zip', overwrite=TRUE)
  }
```

-   We will work with these data sets

# Load the data and explore it!

```{r eval=FALSE, echo=TRUE}
library(tidyverse)

streams <- read_csv('streams.csv')
weather <- read_csv('weather.csv')
socials <- read_csv('socials.csv')
playlists <- read_csv('playlists.csv')
```

- What is the unit of observation?
- Are there any missing values?
- Common columns across data sets?
- We have used:
  - exploring data (`head`, `View()`, `summary()`, `table()`, `dim()`
  -  wrangling with tidyverse/dplyr: `group_by()`, `summarize()`, `mutate()`, `filter()`, `select()`, `arrange()`, `count()`

# Expanding our R knowldege
- But... what about more complex operations?
  - custom summary statistics? ranking? &rarr; create charts 
  - removing duplicates? &rarr; for data auditing
  - complex text searches? &rarr; find artists
  - dealing with time series data? &rarr; replace missings
  - reshaping data? &rarr; bring data into the right shape for analysis
  - merging multiple data sets? &rarr; build "final" data set
  
# Example: mutate and working with dates
- If we run a regression analysis, and we want to account for time, how should the data look like?

# Example: mutate and working with dates

```{r eval=FALSE, echo=TRUE}
# Example 1: mutate and working with dates
library(lubridate)

streams <- streams %>%
  mutate(
    week = week(date),
    week2 = ISOweek::ISOweek(date),
    month = month(date),
    year = year(date)
  )

streams %>% 
  select(date, week, week2, month, year) %>%
  head(10)
```

# Example: Filling missing data
- If we have missing data and we want to impute it, how can we do that? And how should we do it?

# Example: Filling missing data

```{r eval=FALSE, echo=TRUE}
tmp_dates=seq(from=as.Date('2025-01-01'),
              length.out=10, by='1 day')

sim_data= tibble(date=tmp_dates, 
                 price = c(1.20,1.10,NA,1.10,1.5,
                           1.5,NA, NA, 1.4, 1.3),
                 playlists = c(30,20,NA,10,NA,
                               NA, 30, 40, NA, NA))
```

# Example: Filling missing data

```{r eval=FALSE, echo=TRUE}
library(zoo)

sim_data = sim_data %>% 
  mutate(prices_imputed = zoo::na.locf(price, na.rm=F))
sim_data
```

# Example, pivot longer & pivot wider
- What is meant by pivot longer and pivot wider?
- Why would we want to do these things?

# Example: Pivot longer

```{r eval=FALSE, echo=TRUE}
long_weather <- weather %>%
  pivot_longer(cols = ends_with('temp'), 
               names_to = "name", 
               values_to = "temperature") %>%
  separate(name, 
           into = c('country','variable'), 
           sep='_') %>% 
  select(date, country, temperature)

head(long_weather)
```

# Example: Pivot wider

```{r eval=FALSE, echo=TRUE}
wide_weather <- long_weather %>% 
  pivot_wider(names_from = 'country', 
              values_from = 'temperature')

head(wide_weather)
```

# Example: merging data
- Big companies have loads of different data, in different tables
- They need to combine different data sets to analyze the data they need
- How does merging work?

# Example: merging data

```{r eval=FALSE, echo=TRUE}
# For this exercise, let's add a week column that we can merge on
streams <- streams %>%
  mutate(week=ISOweek::ISOweek(date))

merged_data <- streams %>%
  left_join(playlists, 
            by = c("artist" = "artist",
                   "week"="iso_week"))  
# Keeps all stream data, adds playlist info.

# Note: here we specify that "week" in the LEFT data frame 
#(streams) is matched on the "iso_week" 
#column in the RIGHT data frame (playlists). 

head(merged_data)
```

# Let's do some exercises, Exercise 1 & 2

-   Load the data
-   Convert daily streams into weekly aggregates using ISO weeks. Take the mean().

# Exercise 3:Add a trend variable

-   Add a trend variable that increases from 1 (for the first week) to the last.

# Exercise 4: Create a Complete Data Grid

-   Ensure that every artist-country-week combination is represented, even if no streams were recorded.

# Exercise 5: Merge Streams Data into the Complete Grid (complete_data)

-   Join the previous dataset with the complete data set from exercise 4.

# Exercise 6: Merge Weather and Social Media data

-   Prepare merge weather data
    -   First, create an ISO week column.
    -   Then, aggregate to the weekly level.
    -   Next, turn it back into wide (with temp and sun as columns)
    -   Finally, you can merge it with complete_data.
-   Prepare and merge socials data
    -   Add ISO week to socials data
    -   Aggregate to weekly level (week, artist, platform and metric)
    -   Reshape long to wide
    -   Merge!

# Complete at home

-   Exercises 6 through 9, complete at home!

